{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\text{Training GNNs on Large Graphs}$\n",
    "\n",
    "실제로 우리가 다루는 그래프는 엄청난 양의 데이터를 가지고 있습니다.\n",
    "\n",
    "그렇기에 그래프의 node와 edge를 다루기에는 엄청난 양의 저장 공간이 요구됩니다. \n",
    "\n",
    "만약 GPU를 사용해서 연산 속도를 향상시키고자 하는 경우, GPU에서 전체 그래프의 훈련이 불가능한 경우가 많습니다. (데이터가 너무 많아서.)\n",
    "\n",
    "따라서, 본 tutorial에서는 위와 같은 문제를 해결하고자 다음과 같은 두 가지 방법을 다룹니다. \n",
    "\n",
    "1. Stochastic training on graphs.\n",
    "2. Neighbor sampling on graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\EonKim\\anaconda3\\envs\\dgl\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import dgl \n",
    "import dgl.function as fn \n",
    "from dgl.nn.pytorch import conv as dgl_conv \n",
    "\n",
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\text{Mini-batch Construction from a Graph} $\n",
    "\n",
    "일반적인 딥러닝에서는 stochastic training을 위해 학습용 데이터를 mini-batch 형태로 분할하고 각 단계에 필요한 정보만 GPU에 넣어서 연산을 진행합니다. \n",
    "\n",
    "Node Classification을 수행하는 경우에는 다음과 같습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx \n",
    "\n",
    "example_graph_nx = nx.Graph(\n",
    "    [(0, 2), (0, 4), (0, 6), (0, 7), (0, 8), (0, 9), (0, 10), \n",
    "     (1, 2), (1, 3), (1, 5), (2, 3), (2, 4), (2, 6), (3, 5),\n",
    "     (3, 8), (4, 7), (8, 9), (8, 11), (9, 10), (9, 11)]\n",
    ")\n",
    "\n",
    "example_graph = dgl.from_networkx(example_graph_nx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image1](asset/figure_1.JPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\text{Single Layer} $\n",
    "\n",
    "GraphSAGE layer를 사용하여 node 4와 6의 output representation을 계산하고자 하는 경우, node 4와 6의 input feature와 이웃에 대한 input feature 필요합니다. \n",
    "\n",
    "위 예제의 경우 node 4와 6의 feature, 그리고 두 node의 이웃인 node 0, 2, 7에 대한 feature도 요구됩니다.\n",
    "\n",
    "위 feature를 mini-batch 구조로 만들기 위해 DGL API: `dgl.sample_neighbors`를 사용합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|V|=12 |E|=4\n",
      "7 4\n",
      "0 4\n",
      "0 6\n",
      "2 6\n"
     ]
    }
   ],
   "source": [
    "sampled_node_batch = torch.LongTensor([4, 6])\n",
    "sampled_graph = dgl.sampling.sample_neighbors(example_graph, sampled_node_batch, 2)\n",
    "print(f'|V|={sampled_graph.number_of_nodes()} |E|={sampled_graph.number_of_edges()}')\n",
    "src, dst = sampled_graph.all_edges()\n",
    "\n",
    "for s, d in zip(src, dst):\n",
    "    print(s.numpy(), d.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`DGL`은 데이터 구조를 잘 반영하기 위해 `block`을 제공합니다. sub graph는 `dgl.to_block` 함수를 사용하여 `block`으로 쉽게 변환할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#source: 5\n",
      "Node ID of source nodes in original graph: tensor([4, 6, 7, 0, 2])\n",
      "#destination: 2\n",
      "Node ID of destination nodes in original graph: tensor([4, 6])\n",
      "edges in local node ids\n",
      "2 0\n",
      "3 0\n",
      "3 1\n",
      "4 1\n",
      "edges in the original node ids\n",
      "7 4\n",
      "0 4\n",
      "0 6\n",
      "2 6\n"
     ]
    }
   ],
   "source": [
    "sampled_block = dgl.to_block(sampled_graph, sampled_node_batch)\n",
    "\n",
    "def print_block_info(sampled_block):\n",
    "    print('#source:', sampled_block.number_of_src_nodes())\n",
    "    sampled_input_nodes = sampled_block.srcdata[dgl.NID]\n",
    "    print('Node ID of source nodes in original graph:', sampled_input_nodes)\n",
    "    \n",
    "    sampled_output_nodes = sampled_block.dstdata[dgl.NID]\n",
    "    print('#destination:', sampled_block.number_of_dst_nodes())\n",
    "    print('Node ID of destination nodes in original graph:', sampled_output_nodes)\n",
    "    \n",
    "    sampled_block_edges_src, sampled_block_edges_dst = sampled_block.all_edges()\n",
    "    print('edges in local node ids')\n",
    "    \n",
    "    for s, d in zip(sampled_block_edges_src, sampled_block_edges_dst):\n",
    "        print(s.numpy(), d.numpy())\n",
    "    \n",
    "    sampled_block_edges_src_mapped = sampled_input_nodes[sampled_block_edges_src]\n",
    "    sampled_block_edges_dst_mapped = sampled_output_nodes[sampled_block_edges_dst]\n",
    "    print('edges in the original node ids')\n",
    "    for s, d in zip(sampled_block_edges_src_mapped, sampled_block_edges_dst_mapped):\n",
    "        print(s.numpy(), d.numpy())\n",
    "    \n",
    "print_block_info(sampled_block)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\text{Multiple Layers} $\n",
    "\n",
    "지금부터 2-layer GraphSAGE를 사용하여 node 4와 6의 output을 계산하려고 합니다. \n",
    "\n",
    "node 4와 6의 output을 계산하기 위해서는 전체 그래프의 데이터가 필요한 것이 아니고, node 4, 6의 feature 그리고, 4, 6의 이웃 node가 필요합니다. \n",
    "\n",
    "따라서, 우리는 `NeighborSampler`를 통해 이웃 node를 sampling하고자 합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeighborSampler(object):\n",
    "    def __init__(self, g, num_fanouts):\n",
    "        \"\"\"\n",
    "        num_fanouts : list of fanout on each layer.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.g = g \n",
    "        self.num_fanouts = num_fanouts \n",
    "        \n",
    "    def sample(self, seeds):\n",
    "        seeds = torch.LongTensor(self.num_fanouts)\n",
    "        blocks = []\n",
    "        for fanout in reversed(self.num_fanouts):\n",
    "            if fanout >= self.g.number_of_nodes():\n",
    "                sampled_graph = dgl.in_subgraph(self.g, seeds)\n",
    "            \n",
    "            else:\n",
    "                sampled_graph = dgl.sampling.sample_neighbors(self.g, seeds, fanout)\n",
    "            \n",
    "            sampled_block = dgl.to_block(sampled_graph, seeds)\n",
    "            seeds = sampled_block.srcdata[dgl.NID]\n",
    "            blocks.insert(0, sampled_block)\n",
    "        return blocks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#blocks: 2\n",
      "Block for first layer\n",
      "---------------------\n",
      "#source: 8\n",
      "Node ID of source nodes in original graph: tensor([2, 1, 0, 3, 4, 5, 9, 8])\n",
      "#destination: 4\n",
      "Node ID of destination nodes in original graph: tensor([2, 1, 0, 3])\n",
      "edges in local node ids\n",
      "3 0\n",
      "4 0\n",
      "0 1\n",
      "5 1\n",
      "6 2\n",
      "0 2\n",
      "5 3\n",
      "7 3\n",
      "edges in the original node ids\n",
      "3 2\n",
      "4 2\n",
      "2 1\n",
      "5 1\n",
      "9 0\n",
      "2 0\n",
      "5 3\n",
      "8 3\n",
      "\n",
      "Block for second layer\n",
      "----------------------\n",
      "#source: 4\n",
      "Node ID of source nodes in original graph: tensor([2, 1, 0, 3])\n",
      "#destination: 1\n",
      "Node ID of destination nodes in original graph: tensor([2, 2])\n",
      "edges in local node ids\n",
      "1 0\n",
      "2 0\n",
      "2 0\n",
      "3 0\n",
      "edges in the original node ids\n",
      "1 2\n",
      "0 2\n",
      "0 2\n",
      "3 2\n"
     ]
    }
   ],
   "source": [
    "block_sampler = NeighborSampler(example_graph, [2, 2])\n",
    "sampled_blocks = block_sampler.sample(sampled_node_batch)\n",
    "\n",
    "print('#blocks:', len(sampled_blocks))\n",
    "print('Block for first layer')\n",
    "print('---------------------')\n",
    "print_block_info(sampled_blocks[0])\n",
    "print('\\nBlock for second layer')\n",
    "print('----------------------')\n",
    "print_block_info(sampled_blocks[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $ \\text{Minibatch training for 2-layer GraphSage} $\n",
    "\n",
    "GraphSAGE on blocks \n",
    "\n",
    "sampled block은 bipartite graph 입니다. 이를 구축하기 위해서는 `dgl`의 `SAGEConv`를 사용하면 쉽게 구축이 가능합니다. \n",
    "\n",
    "`dgl`은 bipartite graph 만 지원하는 것 뿐만 아니라 homogenouse graph도 지원하기 때문에 목적에 맞게 사용할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl.nn as dglnn\n",
    "import torch.nn.functional as F \n",
    "\n",
    "class SAGENet(nn.Module):\n",
    "    def __init__(self, n_layers, in_feats, out_feats, hidden_feats=None):\n",
    "        super().__init__()\n",
    "        self.convs = nn.ModuleList()\n",
    "        \n",
    "        if hidden_feats is None:\n",
    "            hidden_feats = out_feats \n",
    "        \n",
    "        if n_layers == 1:\n",
    "            self.convs.append(dglnn.SAGEConv(in_feats, out_feats, 'mean'))\n",
    "        \n",
    "        else:\n",
    "            self.convs.append(dglnn.SAGEConv(in_feats, hidden_feats, 'mean', activation=F.relu))\n",
    "            for i in range(n_layers -2):\n",
    "                self.convs.append(dglnn.SAGEConv(hidden_feats, hidden_feats, 'mean', activation=F.relu))\n",
    "            self.convs.append(dglnn.SAGEConv(hidden_feats, out_feats, 'mean', activation=F.relu))\n",
    "        \n",
    "    def forward(self, blocks, input_features):\n",
    "        \"\"\"\n",
    "        blocks: List of blocks generated block sampler.\n",
    "        input_features: Input feature of the first block.\n",
    "        \"\"\"\n",
    "        h = input_features \n",
    "        for layer, block in zip(self.convs, blocks):\n",
    "            h = self.propagate(block, h, layer)\n",
    "        return h \n",
    "    \n",
    "    def propagate(self, block, src_feats, layer):\n",
    "        '''\n",
    "        graphSAGE를 적용하기 위해서는 이웃 feature 뿐만 아니라 node에 대한 feature도 필요합니다. \n",
    "        따라서, 현재 layer에 있는 output node의 feature를 복사한 후 사용하여야 합니다. \n",
    "        block의 ouptut node는 input node에서 첫 번째로 나타나기에 아래와 같은 코드로 사용이 가능합니다. \n",
    "        '''\n",
    "        dst_feats = src_feats[:block.number_of_dst_nodes()]\n",
    "        return layer(block, (src_feats, dst_feats))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\text{Inference with mini-batch} $\n",
    "\n",
    "mini-batch fashion에서 inference를 하기 위해서는 먼저, 첫 번째 GraphSAGE layer에서 노드에 대한 representation을 계산하여야 합니다.\n",
    "\n",
    "$1^{\\text{th}}$-layer에서 계산한 representation을 바탕으로 $2^{\\text{nd}}$-layer를 계산합니다. 이와 같은 과정을 last layer까지 반복한 후 결과를 산출합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_with_sagenet(sagenet, graph, input_features, batch_size):\n",
    "    block_sampler = NeighborSampler(graph, [graph.number_of_nodes()])\n",
    "    h = input_features\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for conv in sagenet.convs:\n",
    "            new_h_list = []\n",
    "            node_ids = torch.arange((graph.number_of_nodes()))\n",
    "            \n",
    "            for batch_start in range(0, graph.number_of_nodes(), batch_size):\n",
    "                block = block_sampler.sample(node_ids[batch_start:batch_start+batch_size])[0]\n",
    "                input_node_ids = block.srcdata[dgl.NID]\n",
    "                \n",
    "                h_input = h[input_node_ids]\n",
    "                \n",
    "                new_h = sagenet.propagate(block, h_input, conv)\n",
    "                new_h_list.append(new_h)\n",
    "            \n",
    "            h = torch.cat(new_h_list)\n",
    "    \n",
    "    return h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $ \\text{Load Dataset} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  NumNodes: 19717\n",
      "  NumEdges: 88651\n",
      "  NumFeats: 500\n",
      "  NumClasses: 3\n",
      "  NumTrainingSamples: 60\n",
      "  NumValidationSamples: 500\n",
      "  NumTestSamples: 1000\n",
      "Done loading data from cached files.\n"
     ]
    }
   ],
   "source": [
    "import dgl.data \n",
    "\n",
    "dataset = dgl.data.citation_graph.load_pubmed()\n",
    "\n",
    "graph = dataset[0]\n",
    "\n",
    "\n",
    "in_feats = graph.ndata['feat'].shape[1]\n",
    "num_labels = dataset.num_classes\n",
    "\n",
    "train_nid = torch.where(graph.ndata['train_mask'])[0]\n",
    "val_nid = torch.where(graph.ndata['val_mask'])[0]\n",
    "test_nid = torch.where(graph.ndata['test_mask'])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbor_sampler = NeighborSampler(graph, [10, 25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader \n",
    "import torch.optim as optim \n",
    "import torch.nn as nn \n",
    "\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "train_dataloader = DataLoader(train_nid, batch_size = BATCH_SIZE, collate_fn=neighbor_sampler.sample, shuffle=True)\n",
    "\n",
    "HIDDEN_FEATURES = 50 \n",
    "model = SAGENet(2, in_feats, num_labels, HIDDEN_FEATURES)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\text{Evaluation} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_accuracy(pred, true):\n",
    "    pred = torch.argmax(pred, dim=1)\n",
    "    return (pred == true).float().mean().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $ \\text{Training Loop} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 10\n",
    "EVAL_BATCH_SIZE = 10\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    for blocks in train_dataloader:\n",
    "        input_nodes = blocks[0].srcdata[dgl.NID]\n",
    "        output_nodes = blocks[-1].dstdata[dgl.NID]\n",
    "        \n",
    "        input_features = graph.ndata['feat'][input_nodes]\n",
    "        output_labels = graph.ndata['label'][output_nodes]\n",
    "        \n",
    "        output_pred = model(blocks, input_features)\n",
    "        loss = criterion(output_pred, output_labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    \n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        model.eval()\n",
    "        all_predictions = inference_with_sagenet(model, graph, graph.ndata['feat'], EVAL_BATCH_SIZE)\n",
    "        \n",
    "        val_pred = all_predictions[val_nid]\n",
    "        val_labels = graph.ndata['labels'][val_nid]\n",
    "        test_pred = all_predictions[test_nid]\n",
    "        test_labels = graph.ndata['labels'][test_nid]\n",
    "        \n",
    "        print('Validation acc: ', calc_accuracy(val_pred, val_labels), 'Test acc: ', calc_accuracy(test_pred, test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('dgl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "91f4586c667beba9fc73d5e38cfe2361778c9000e218f655761c33977cb8e239"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
