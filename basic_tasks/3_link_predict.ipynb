{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\text{Link Prediction using Graph Neural Network} $\n",
    "\n",
    "kipf's 가 제안한 GCN, 이후 나온 GraphSAGE 등의 모델은 Node Classification을 위한 모델입니다. \n",
    "\n",
    "본 tutorial에서는 Link Prediction을 하기 위한 workflow를 다룹니다. \n",
    "\n",
    "* Prepare training and testing sets for link prediction task.\n",
    "* Build a GNN-based link prediction model.\n",
    "* Train the model and verify the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl \n",
    "import time \n",
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.optim as optim \n",
    "import torch.nn.functional as F \n",
    "\n",
    "import numpy as np \n",
    "import scipy.sparse as sp "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load_zachery\n",
    "\n",
    "g = load_zachery()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\text{Prepare training and testing set} $\n",
    "\n",
    "`edge`에는 두 가지 종류가 있습니다. `negative`, `positive`.. \n",
    "\n",
    "예제에서는 모델의 학습을 위해 train edge와 test edge를 분할합니다. 50개는 test, 나머지는 train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "u, v = g.edges()\n",
    "eids = np.arange(g.number_of_edges())\n",
    "eids = np.random.permutation(eids)\n",
    "\n",
    "test_pos_u, test_pos_v = u[eids[:50]], v[eids[:50]]\n",
    "train_pos_u, train_pos_v = u[eids[50:]], v[eids[50:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coo_matrix를 사용하는 이유는 matrix에 0이 많은 경우 압축률이 좋기 때문입니다. \n",
    "# adj = g.adjacency_matrix(transpose = False, scipy_fmt='csr').todense() 둘 중 아무거나 사용하셔도 무방합니다. \n",
    "\"\"\"\n",
    "edge가 없는 부분을 negative로 표현하기 위해 이러한 과정을 수행합니다. \n",
    "\"\"\"\n",
    "adj = sp.coo_matrix((np.ones(len(u)), (u.numpy(), v.numpy()))) # edge에 weight를 따로 지정하지 않았기 때문에 np.ones(len(u))를 사용합니다. \n",
    "adj_neg = 1 - adj.todense() - np.eye(34) # np.eye == digonal matrix \n",
    "neg_u, neg_v = np.where(adj_neg != 0)\n",
    "neg_eids = np.random.choice(len(neg_u), 200)\n",
    "test_neg_u, test_neg_v = neg_u[neg_eids[:50]], neg_v[neg_eids[:50]]\n",
    "train_neg_u, train_neg_v = neg_u[neg_eids[50:]], neg_v[neg_eids[50:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_u = torch.cat([torch.as_tensor(train_pos_u), torch.as_tensor(train_neg_u)])\n",
    "train_v = torch.cat([torch.as_tensor(train_pos_v), torch.as_tensor(train_neg_v)])\n",
    "train_label = torch.cat([torch.zeros(len(train_pos_u)), torch.ones(len(train_neg_u))])\n",
    "\n",
    "test_u = torch.cat([torch.as_tensor(test_pos_u), torch.as_tensor(test_neg_u)])\n",
    "test_v = torch.cat([torch.as_tensor(test_pos_v), torch.as_tensor(test_neg_v)])\n",
    "test_label = torch.cat([torch.zeros(len(test_pos_u)), torch.ones(len(test_neg_u))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $ \\text{Define a GraphSAGE model} $\n",
    "\n",
    "### $$ h^k_{\\mathcal{N}(v)} \\leftarrow \\text{AGGEGATE}_k(h^{k-1}_u, \\forall u \\in \\mathcal{N}(v))$$\n",
    "### $$ h^k_v \\leftarrow \\sigma(W^k \\cdot \\text{CONCAT} (h^{k-1}_v, h^k_{N(v)})) $$\n",
    "\n",
    "DGL은 많은 neighbor aggregation modules을 제공하고 있습니다. 사용하고자 하는 module을 호출(invoke)해서 사용하시면 됩니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dgl.nn import SAGEConv \n",
    "# build a two-layer GraphSAGE model \n",
    "class GraphSAGE(nn.Module):\n",
    "    def __init__(self, num_nodes, embed_dim, h_feats):\n",
    "        super(GraphSAGE, self).__init__()\n",
    "        self.num_nodes = num_nodes \n",
    "        self.embeded = nn.Embedding(num_nodes, embed_dim)\n",
    "        self.conv1 = SAGEConv(embed_dim, h_feats, 'mean')\n",
    "        self.conv2 = SAGEConv(h_feats, h_feats, 'mean')\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        self._init_weight()\n",
    "        \n",
    "    def forward(self, g):\n",
    "        embed = self.embeded.weight\n",
    "        output = self.conv1(g, embed)\n",
    "        output = self.relu(output)\n",
    "        output = self.conv2(g, output)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def _init_weight(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Embedding):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "\n",
    "\n",
    "num_nodes = g.number_of_nodes()\n",
    "embed_dim = 5\n",
    "h_feats = 16\n",
    "models = GraphSAGE(num_nodes, embed_dim, h_feats)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then optimize the model using the following loss function.\n",
    "\n",
    "$$ \\hat{y}_{u~v} = \\sigma(h^T_u h_v) $$\n",
    "$$ \\mathcal{L} = - \\sum_{u~v \\in \\mathcal{D}} ( y_{u~v} \\log (\\hat{y}_{u~v}) + (1-y_{u~v}) \\log (1 - \\hat{y}_{u~v})) $$\n",
    "\n",
    "기본적으로 위에서 구축한 모델은 두 노드의 표현(representation)을 내적하여 edge score를 예측합니다. \n",
    "\n",
    "그 후, target $y$가 0 혹은 1인 binary cross entropy loss를 계산하여 edge가 양수인지 확인합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(models.parameters(), lr = 1e-2)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "def calc_accuracy(pred, true):\n",
    "    return ((pred >= 0.5) == true).sum().item() / len(pred)\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time \n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = elapsed_time - elapsed_mins * 60 \n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [5/100] | elapsed time 0m, 0.03s\n",
      "train loss: 0.5798\t train acc: 66.80%\n",
      "test loss: 0.6922\t train acc: 53.00% \n",
      "\n",
      "epoch [10/100] | elapsed time 0m, 0.01s\n",
      "train loss: 0.4882\t train acc: 78.52%\n",
      "test loss: 0.6721\t train acc: 61.00% \n",
      "\n",
      "epoch [15/100] | elapsed time 0m, 0.01s\n",
      "train loss: 0.4100\t train acc: 79.69%\n",
      "test loss: 0.6300\t train acc: 67.00% \n",
      "\n",
      "epoch [20/100] | elapsed time 0m, 0.01s\n",
      "train loss: 0.3209\t train acc: 85.55%\n",
      "test loss: 0.5630\t train acc: 68.00% \n",
      "\n",
      "epoch [25/100] | elapsed time 0m, 0.01s\n",
      "train loss: 0.2500\t train acc: 89.06%\n",
      "test loss: 0.5799\t train acc: 70.00% \n",
      "\n",
      "epoch [30/100] | elapsed time 0m, 0.01s\n",
      "train loss: 0.1843\t train acc: 91.41%\n",
      "test loss: 0.6130\t train acc: 75.00% \n",
      "\n",
      "epoch [35/100] | elapsed time 0m, 0.01s\n",
      "train loss: 0.1288\t train acc: 95.70%\n",
      "test loss: 0.6224\t train acc: 80.00% \n",
      "\n",
      "epoch [40/100] | elapsed time 0m, 0.01s\n",
      "train loss: 0.0792\t train acc: 96.88%\n",
      "test loss: 0.7371\t train acc: 88.00% \n",
      "\n",
      "epoch [45/100] | elapsed time 0m, 0.01s\n",
      "train loss: 0.0448\t train acc: 99.22%\n",
      "test loss: 0.9246\t train acc: 86.00% \n",
      "\n",
      "epoch [50/100] | elapsed time 0m, 0.01s\n",
      "train loss: 0.0236\t train acc: 100.00%\n",
      "test loss: 2.7407\t train acc: 89.00% \n",
      "\n",
      "epoch [55/100] | elapsed time 0m, 0.01s\n",
      "train loss: 0.0110\t train acc: 100.00%\n",
      "test loss: 2.8284\t train acc: 89.00% \n",
      "\n",
      "epoch [60/100] | elapsed time 0m, 0.01s\n",
      "train loss: 0.0048\t train acc: 100.00%\n",
      "test loss: 2.9066\t train acc: 89.00% \n",
      "\n",
      "epoch [65/100] | elapsed time 0m, 0.01s\n",
      "train loss: 0.0021\t train acc: 100.00%\n",
      "test loss: 2.9614\t train acc: 88.00% \n",
      "\n",
      "epoch [70/100] | elapsed time 0m, 0.01s\n",
      "train loss: 0.0010\t train acc: 100.00%\n",
      "test loss: 4.6813\t train acc: 88.00% \n",
      "\n",
      "epoch [75/100] | elapsed time 0m, 0.01s\n",
      "train loss: 0.0006\t train acc: 100.00%\n",
      "test loss: 4.7080\t train acc: 88.00% \n",
      "\n",
      "epoch [80/100] | elapsed time 0m, 0.01s\n",
      "train loss: 0.0004\t train acc: 100.00%\n",
      "test loss: 4.7320\t train acc: 88.00% \n",
      "\n",
      "epoch [85/100] | elapsed time 0m, 0.01s\n",
      "train loss: 0.0003\t train acc: 100.00%\n",
      "test loss: 4.7526\t train acc: 88.00% \n",
      "\n",
      "epoch [90/100] | elapsed time 0m, 0.01s\n",
      "train loss: 0.0002\t train acc: 100.00%\n",
      "test loss: 4.7696\t train acc: 88.00% \n",
      "\n",
      "epoch [95/100] | elapsed time 0m, 0.01s\n",
      "train loss: 0.0002\t train acc: 100.00%\n",
      "test loss: 4.7837\t train acc: 88.00% \n",
      "\n",
      "epoch [100/100] | elapsed time 0m, 0.01s\n",
      "train loss: 0.0001\t train acc: 100.00%\n",
      "test loss: 4.7949\t train acc: 88.00% \n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_logits = []\n",
    "num_epochs = 100\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    \n",
    "    models.train()\n",
    "    start_time = time.time()\n",
    "    pred = models(g)\n",
    "    optimizer.zero_grad()\n",
    "    pred_proba = torch.sigmoid((pred[train_u] * pred[train_v]).sum(dim=1))\n",
    "    train_loss = criterion(pred_proba, train_label)\n",
    "    \n",
    "    train_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    all_logits.append(pred_proba.detach())\n",
    "    train_acc = calc_accuracy(pred_proba, train_label)\n",
    "    \n",
    "    if epoch % 5 == 0 :\n",
    "        with torch.no_grad():\n",
    "            models.eval()\n",
    "            pred = models(g)\n",
    "            test_proba = torch.sigmoid((pred[test_u] * pred[test_v]).sum(dim=1))\n",
    "            test_loss = criterion(test_proba, test_label)\n",
    "            \n",
    "            test_acc = calc_accuracy(test_proba, test_label)\n",
    "            \n",
    "            end_time = time.time()\n",
    "            elapsed_mins, elapsed_secs = epoch_time(start_time, end_time)\n",
    "            print(f'epoch [{epoch}/{num_epochs}] | elapsed time {elapsed_mins}m, {elapsed_secs:.2f}s')\n",
    "            print(f'train loss: {train_loss:.4f}\\t train acc: {train_acc*100:.2f}%')\n",
    "            print(f'test loss: {test_loss:.4f}\\t train acc: {test_acc*100:.2f}% \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('dgl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "91f4586c667beba9fc73d5e38cfe2361778c9000e218f655761c33977cb8e239"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
