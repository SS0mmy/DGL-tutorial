{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Training of GNN for Node Classification on Large Graphs \n",
    "\n",
    "This tutorial's content include \n",
    "- Creating your DGL graph from your own data in other formats such as CSV.\n",
    "- Training a GNN model with a single machine, a single GPU, on a graph of any size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-08-30 05:17:38--  https://snap.stanford.edu/ogb/data/nodeproppred/products.zip\n",
      "Resolving snap.stanford.edu (snap.stanford.edu)... 171.64.75.80\n",
      "Connecting to snap.stanford.edu (snap.stanford.edu)|171.64.75.80|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1480993786 (1.4G) [application/zip]\n",
      "Saving to: ‘products.zip’\n",
      "\n",
      "products.zip        100%[===================>]   1.38G  4.22MB/s    in 3m 59s  \n",
      "\n",
      "2022-08-30 05:21:37 (5.92 MB/s) - ‘products.zip’ saved [1480993786/1480993786]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://snap.stanford.edu/ogb/data/nodeproppred/products.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  products.zip\n",
      "   creating: products/\n",
      "   creating: products/split/\n",
      "   creating: products/split/sales_ranking/\n",
      "  inflating: products/split/sales_ranking/test.csv.gz  \n",
      "  inflating: products/split/sales_ranking/train.csv.gz  \n",
      "  inflating: products/split/sales_ranking/valid.csv.gz  \n",
      "   creating: products/processed/\n",
      "   creating: products/raw/\n",
      "  inflating: products/raw/node-label.csv.gz  \n",
      " extracting: products/raw/num-node-list.csv.gz  \n",
      " extracting: products/raw/num-edge-list.csv.gz  \n",
      "  inflating: products/raw/node-feat.csv.gz  \n",
      "  inflating: products/raw/edge.csv.gz  \n",
      "   creating: products/mapping/\n",
      "  inflating: products/mapping/README.md  \n",
      " extracting: products/mapping/labelidx2productcategory.csv.gz  \n",
      "  inflating: products/mapping/nodeidx2asin.csv.gz  \n",
      "  inflating: products/RELEASE_v1.txt  \n"
     ]
    }
   ],
   "source": [
    "!unzip -o products.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `products/raw/edge.csv` (source-destination pairs)\n",
    "- `products/raw/node-feat.csv` (node features)\n",
    "- `products/raw-node-label.csv` (node label)\n",
    "- `products/raw/num-edge-list.csv` (number of edges)\n",
    "- `product/raw/num-node-list.csv` (number of nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "edges = pd.read_csv('products/raw/edge.csv.gz', header = None).values\n",
    "node_features = pd.read_csv('products/raw/node-feat.csv.gz', header = None).values \n",
    "node_labels = pd.read_csv('products/raw/node-label.csv.gz', header=None).values[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_nids = pd.read_csv('products/split/sales_ranking/train.csv.gz', header=None).values[:,0]\n",
    "valid_nids = pd.read_csv('products/split/sales_ranking/valid.csv.gz', header=None).values[:,0]\n",
    "test_nids = pd.read_csv('products/split/sales_ranking/test.csv.gz', header=None).values[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DGL backend not selected or invalid.  Assuming PyTorch for now.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting the default backend to \"pytorch\". You can change it in the ~/.dgl/config.json file or export the DGLBACKEND environment variable.  Valid options are: pytorch, mxnet, tensorflow (all lowercase)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "import dgl \n",
    "import torch \n",
    "\n",
    "\n",
    "graph = dgl.graph((edges[:, 0], edges[:, 1]))\n",
    "node_features = torch.FloatTensor(node_features)\n",
    "node_labels = torch.LongTensor(node_labels)\n",
    "\n",
    "with open('data.pkl', 'wb') as f:\n",
    "    pickle.dump((graph, node_features, node_labels, train_nids, valid_nids, test_nids), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph\n",
      "Graph(num_nodes=2449029, num_edges=61859140,\n",
      "      ndata_schemes={}\n",
      "      edata_schemes={})\n",
      "Shape of node features: torch.Size([2449029, 100])\n",
      "Shape of node labels: torch.Size([2449029])\n",
      "Number of classis: 47\n"
     ]
    }
   ],
   "source": [
    "print('Graph')\n",
    "print(graph)\n",
    "print('Shape of node features:', node_features.shape)\n",
    "print('Shape of node labels:', node_labels.shape)\n",
    "\n",
    "num_features = node_features.shape[1]\n",
    "num_classes = (node_labels.max() + 1).item()\n",
    "print('Number of classis:', num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a Data Loader with Neighbor Sampling \n",
    "\n",
    "#### Neighbor sampling overview \n",
    "\n",
    "The formulation of message passing usually has the following form:\n",
    "\n",
    "$$ a^{(l)}_v = \\rho^{(l)} (\\{ h^{(l-1)}_u:u \\in \\mathcal{N}(v) \\}) $$\n",
    "$$ h^{(l)}_v = \\phi^{(l)} (h^{(l-1)}_v, a^{(l)}_v) $$\n",
    "\n",
    "where $\\rho^{(l)}$ and $\\phi^{(l)}$ are parameterized functions, and $\\mathcal{N}(v)$ represents the set of predecessors (or equivalently neighbors) of $v$ on graph $\\mathcal{G}$\n",
    "\n",
    "$$ \\mathcal{N}(v) = \\{ s(e):e\\in \\mathbb{E}, t(e) = v \\} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = dgl.dataloading.MultiLayerNeighborSampler([4, 4, 4]) # MultiLayerNeighborSampler K-hop high-order\n",
    "train_dataloader = dgl.dataloading.NodeDataLoader(\n",
    "    graph, train_nids, sampler, \n",
    "    batch_size = 1024, \n",
    "    shuffle = True, \n",
    "    drop_last = False, \n",
    "    num_workers = 0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([50604,  2334, 72881,  ..., 45141, 19313, 28940]), tensor([ 50604,   2334,  72881,  ..., 115477, 139654, 172127]), [Block(num_src_nodes=35054, num_dst_nodes=15761, num_edges=51350), Block(num_src_nodes=15761, num_dst_nodes=4585, num_edges=15978), Block(num_src_nodes=4585, num_dst_nodes=1024, num_edges=3701)]]\n"
     ]
    }
   ],
   "source": [
    "example_minibatch = next(iter(train_dataloader))\n",
    "print(example_minibatch) # input, output, bipartites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To compute 1024 nodes' output we need 35054 nodes' input features\n"
     ]
    }
   ],
   "source": [
    "input_nodes, output_nodes, bipartites = example_minibatch \n",
    "print(f\"To compute {len(output_nodes)} nodes' output we need {len(input_nodes)} nodes' input features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn \n",
    "import torch.optim as optim \n",
    "import torch.nn.functional as F \n",
    "import dgl.nn as dglnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAGE(nn.Module):\n",
    "    def __init__(self, in_feats, n_hidden, n_classes, n_layers):\n",
    "        super(SAGE, self).__init__()\n",
    "        self.n_layers = n_layers \n",
    "        self.n_hidden = n_hidden \n",
    "        self.n_classes = n_classes \n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(dglnn.SAGEConv(in_feats, n_hidden, 'mean'))\n",
    "\n",
    "        for i in range(1, n_layers - 1):\n",
    "            self.layers.append(dglnn.SAGEConv(n_hidden, n_hidden, 'mean'))\n",
    "        self.layers.append(dglnn.SAGEConv(n_hidden, n_classes, 'mean'))\n",
    "\n",
    "    def forward(self, bipartites, x):\n",
    "        for l, (layer, bipartite) in enumerate(zip(self.layers, bipartites)):\n",
    "            x = layer(bipartite, x)\n",
    "            if l != self.n_layers -1 :\n",
    "                x = F.relu(x)\n",
    "        return x "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Training Loop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "model = SAGE(num_features, 128, num_classes, 3).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dataloader = dgl.dataloading.NodeDataLoader(\n",
    "    graph, valid_nids, sampler, \n",
    "    batch_size = 1024, \n",
    "    shuffle = False, \n",
    "    drop_last = False, \n",
    "    num_workers = 2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 193/193 [00:05<00:00, 32.49it/s, loss=0.096, acc=1.000]\n",
      "100%|██████████| 39/39 [00:01<00:00, 30.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Validation Accuracy 88.45%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 193/193 [00:05<00:00, 35.14it/s, loss=0.197, acc=0.857]\n",
      "100%|██████████| 39/39 [00:01<00:00, 29.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Validation Accuracy 88.43%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 193/193 [00:05<00:00, 32.87it/s, loss=0.316, acc=0.857]\n",
      "100%|██████████| 39/39 [00:01<00:00, 29.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Validation Accuracy 88.22%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 193/193 [00:05<00:00, 32.88it/s, loss=0.251, acc=0.857]\n",
      "100%|██████████| 39/39 [00:01<00:00, 29.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Validation Accuracy 88.44%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 193/193 [00:05<00:00, 33.35it/s, loss=0.197, acc=1.000]\n",
      "100%|██████████| 39/39 [00:01<00:00, 29.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Validation Accuracy 88.45%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 193/193 [00:05<00:00, 33.17it/s, loss=0.050, acc=1.000]\n",
      "100%|██████████| 39/39 [00:01<00:00, 29.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 Validation Accuracy 88.76%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 193/193 [00:05<00:00, 33.03it/s, loss=0.603, acc=0.857]\n",
      "100%|██████████| 39/39 [00:01<00:00, 29.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 Validation Accuracy 88.34%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 193/193 [00:05<00:00, 33.43it/s, loss=0.557, acc=0.857]\n",
      "100%|██████████| 39/39 [00:01<00:00, 29.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 Validation Accuracy 88.48%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 193/193 [00:05<00:00, 33.05it/s, loss=0.587, acc=0.857]\n",
      "100%|██████████| 39/39 [00:01<00:00, 29.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 Validation Accuracy 88.29%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 193/193 [00:05<00:00, 33.30it/s, loss=0.395, acc=0.714]\n",
      "100%|██████████| 39/39 [00:01<00:00, 29.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 Validation Accuracy 88.51%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from re import L\n",
    "import tqdm \n",
    "import numpy as np \n",
    "import sklearn.metrics as metrics \n",
    "\n",
    "best_acc = 0 \n",
    "best_model_path = 'model.pt'\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    with tqdm.tqdm(train_dataloader) as tq:\n",
    "        for i, (input_nodes, output_nodes, bipartites) in enumerate(tq):\n",
    "            bipartites = [b.to(device) for b in bipartites]\n",
    "            inputs = node_features[input_nodes].to(device)\n",
    "            labels = node_labels[output_nodes].to(device)\n",
    "            predictions = model(bipartites, inputs)\n",
    "\n",
    "            loss = F.cross_entropy(predictions, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            acc = metrics.accuracy_score(labels.cpu().numpy(), predictions.argmax(1).detach().cpu().numpy())\n",
    "\n",
    "            tq.set_postfix({'loss': '%.03f' %loss.item(), 'acc': '%.03f'%acc}, refresh = False)\n",
    "\n",
    "\n",
    "        \n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    predictions = []\n",
    "    labels = []\n",
    "\n",
    "    with tqdm.tqdm(valid_dataloader) as tq, torch.no_grad():\n",
    "        for (input_nodes, output_nodes, bipartites) in tq:\n",
    "            bipartites = [b.to(device) for b in bipartites]\n",
    "            input_nodes = node_features[input_nodes].to(device)\n",
    "            labels.append(node_labels[output_nodes].numpy())\n",
    "            predictions.append(model(bipartites, input_nodes).argmax(1).cpu().numpy())\n",
    "        \n",
    "        predictions = np.concatenate(predictions)\n",
    "        labels = np.concatenate(labels)\n",
    "        acc = metrics.accuracy_score(labels, predictions)\n",
    "        print(f'Epoch {epoch+1} Validation Accuracy {acc*100:.2f}%')\n",
    "        if best_acc < acc:\n",
    "            best_acc = acc \n",
    "            torch.save(model.state_dict(), best_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model, graph, input_features, batch_size):\n",
    "    nodes = torch.arange(graph.number_of_nodes())\n",
    "\n",
    "    sampler = dgl.dataloading.MultiLayerNeighborSampler([None])\n",
    "    dataloader = dgl.dataloading.NodeDataLoader(\n",
    "        graph, nodes, sampler, \n",
    "        batch_size = batch_size, \n",
    "        shuffle = False, \n",
    "        drop_last = False, \n",
    "        num_worker = 0\n",
    "    )\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for l, layer in enumerate(model.layers):\n",
    "            output_features = torch.zeros(graph.number_of_nodes(), model.n_hidden if l != model.n_layers -1 else model.n_classes)\n",
    "\n",
    "        for input_nodes, output_nodes, bipartites in tqdm.tqdm(dataloader):\n",
    "            bipartite = bipartites[0].to(device)\n",
    "\n",
    "            x = input_features[input_nodes].to(device)\n",
    "\n",
    "            x = layer(bipartite, x)\n",
    "            if l != model.n_layers -1:\n",
    "                x = F.relu(x)\n",
    "\n",
    "            output_features[output_nodes] = x.cpu()\n",
    "        input_features = output_features \n",
    "    return output_features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(best_model_path))\n",
    "all_predictions = inference(model, graph, node_features, 8192)\n",
    "\n",
    "test_predictions = all_predictions[test_nids].argmax(1)\n",
    "test_labels = node_labels[test_nids]\n",
    "test_accuracy = metrics.accuracy_score(test_predictions.numpy(), test_labels.numpy())\n",
    "print('Test accuracy:', test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion \n",
    "\n",
    "In this tutorial, you have learned how to train a multi-layer GraphSAGE with neighbor sampling on a large dataset that cannot fit into GPU. \n",
    "\n",
    "The method you have learned can scale to a graph of any size, and works on a single machine with a single GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.11 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
